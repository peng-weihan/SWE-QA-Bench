# SWE-QA

**SWE-QA: Can Language Models Answer Repository-level Code Questions?**
This repository contains code and data for the SWE-QA benchmark, which evaluates language models' ability to answer repository-level code questions across 12 popular Python projects including Django, Flask, Requests, and more.

## ğŸ“ Prompts

The detailed prompt templates used in the paper are in the `supplementary.pdf` file

## ğŸ“Š Dataset

The benchmark dataset is available on Hugging Face:
- **Dataset**: [SWE-QA-Benchmark](https://huggingface.co/datasets/swe-qa/SWE-QA-Benchmark)

## ğŸ“– Paper

For more details about the methodology and results, please refer to the paper:
- **Paper**: "SWE-QA: Can Language Models Answer Repository-level Code Questions?"

## ğŸ“ Repository Structure

```
SWE-QA-Bench/
â”œâ”€â”€ SWE-QA-Bench/                    # Main package directory
â”‚   â”œâ”€â”€ datasets/              # Dataset files and repositories
â”‚   â”‚   â”œâ”€â”€ questions/         # Question datasets (JSONL format)
â”‚   â”‚   â”‚   â”œâ”€â”€ astropy.jsonl  # Project-specific datasets
â”‚   â”‚   â”‚   â”œâ”€â”€ django.jsonl
â”‚   â”‚   â”‚   ...
â”‚   â”‚   â”œâ”€â”€ answers/           # Answer datasets
â”‚   â”‚   â”œâ”€â”€ faiss/             # FAISS index files
â”‚   â”‚   â””â”€â”€ repos/             # Repository data
â”‚   â”œâ”€â”€ issue_analyzer/        # GitHub issue analysis
â”‚   â”‚   â”œâ”€â”€ get_question_from_issue.py
â”‚   â”‚   â””â”€â”€ pull_issues.py
â”‚   â”œâ”€â”€ methods/               # Evaluation methods
â”‚   â”‚   â”œâ”€â”€ llm_direct/        # Direct LLM evaluation
â”‚   â”‚   â”œâ”€â”€ rag_function_chunk/ # RAG with function chunking
â”‚   â”‚   â”œâ”€â”€ rag_sliding_window/ # RAG with sliding window
â”‚   â”‚   â”œâ”€â”€ code_formatting.py
â”‚   â”‚   â””â”€â”€ data_models.py
â”‚   â”œâ”€â”€ score/                 # Scoring utilities
â”‚   â”‚   â””â”€â”€ llm-score.py       # LLM-as-a-judge evaluation
â”‚   â”œâ”€â”€ models/                # Data models
â”‚   â”‚   â””â”€â”€ data_models.py
â”‚   â””â”€â”€ utils/                 # Utility functions
â”œâ”€â”€ docs/                      # Documentation of each part
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ LICENSE                    # License file
â”œâ”€â”€ supplementary.pdf          # Supplementary file (prompts)
â”œâ”€â”€ clone_repos.sh             # Script to clone repositories at specific commits
â”œâ”€â”€ repos.txt                  # List of repository URLs and commit hashes
â”œâ”€â”€ requirements.txt           # Python dependencies required to run the project
â””â”€â”€ README.md                  # This file
```


## ğŸš€ Environment Setup

### Prerequisites

- Python 3.12
- pip or conda for package management
- OpenAI API access (required for all evaluation methods)
- Voyage AI API access (required for RAG-based methods)

### Installation
**Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
   
**SWE Repository Prerequisites:**
   ```bash
   # Use the provided script to clone all repositories at specific commits
   ./clone_repos.sh
   ```

## âš¡ Quick Start

### 1. Direct LLM Evaluation

Before executing, you need to configure the environment variables by filling the `.env` file in the `SWE-QA-Bench/methods/llm_direct` directory:
```bash
OPENAI_BASE_URL=your_openai_base_url
OPENAI_API_KEY=your_api_key
MODEL=your_model_name
```

Evaluate language models directly on repository-level questions:
```bash
cd SWE-QA-Bench/methods/llm_direct
python main.py
```

This method will:
- Load questions from the dataset
- Send questions directly to the LLM
- Generate answers without additional context
- Save results to `datasets/answers/direct/`

### 2. RAG with Function Chunking
Before executing, you need to configure the environment variables by filling the `.env` file in the `SWE-QA-Bench/methods/rag_function_chunk` directory:
```bash
# Voyage AI Configuration
VOYAGE_API_KEY=
VOYAGE_MODEL=  # voyage-code-3 recommended

# OpenAI Configuration
OPENAI_BASE_URL=
OPENAI_API_KEY=
MODEL=
```

Use RAG with function-level code chunking:

```bash
cd SWE-QA-Bench/methods/rag_function_chunk
python main.py
```

This method will:
- Parse code into function-level chunks
- Build vector embeddings for code chunks
- Retrieve relevant code context for each question
- Generate answers using retrieved context

### 3. RAG with Sliding Window

Before executing, you need to configure the environment variables by filling the `.env` file in the `SWE-QA-Bench/methods/rag_sliding_window` directory:
```bash
# Voyage AI Configuration
VOYAGE_API_KEY=
VOYAGE_MODEL=   # voyage-code-3 recommended

# OpenAI Configuration
OPENAI_URL=
OPENAI_KEY=
MODEL=
```

Use RAG with sliding window text chunking:

```bash
cd SWE-QA-Bench/methods/rag_sliding_window
python main.py
```

This method will:
- Split code into overlapping text windows
- Create embeddings for text chunks
- Retrieve relevant chunks for each question
- Generate contextual answers

### 4. Evaluation and Scoring
Before executing, you need to configure the environment variables by filling the `.env` file in the `SWE-QA-Bench/score` directory:
```bash
OPENAI_BASE_URL=your_openai_base_url
OPENAI_API_KEY=your_api_key
MODEL=your_model_name

METHOD= # choose from [direct, func_chunk, sliding_window]
```

Evaluate generated answers using LLM-as-a-judge:
```bash
cd SWE-QA-Bench/score
python llm-score.py
```

## ğŸ“„ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.